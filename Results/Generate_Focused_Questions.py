#!/usr/bin/env python3
"""
Generate Focused Research Question Analysis for Thesis
This script performs a deep-dive analysis on Accuracy and Relevance, and a 
surface-level analysis on other dimensions, using the CSVs generated by 
the main analysis script.
"""

import pandas as pd
import numpy as np

def load_data():
    """Loads all necessary CSV files into a dictionary of dataframes."""
    data_files = {
        'reliability': 'comprehensive_inter_judge_reliability.csv',
        'system': 'enhanced_system_comparison.csv',
        'judge': 'enhanced_judge_comparison.csv',
    }
    dfs = {}
    try:
        for key, filename in data_files.items():
            dfs[key] = pd.read_csv(filename)
        print("‚úÖ All data files loaded successfully.")
    except FileNotFoundError as e:
        print(f"‚ùå Error: Missing necessary CSV file: {e.filename}")
        print("Please ensure 'Analysis_Enhanced.py' has been run successfully first.")
        return None
    return dfs

def deep_dive_accuracy(dfs):
    """Performs and prints the deep-dive analysis for the Accuracy dimension."""
    print("\n\n" + "="*80)
    print("üéØ DEEP DIVE: ACCURACY DIMENSION")
    print("="*80)
    
    reliability_df = dfs['reliability']
    system_df = dfs['system']
    judge_df = dfs['judge']

    # Q1a-Acc: Human Judge Reliability
    print("\nQ1a-Acc: What is the reliability among human judges on Accuracy for RAG answers?")
    print("-" * 70)
    human_acc = reliability_df[
        (reliability_df['analysis_type'] == 'human_reliability') &
        (reliability_df['metric'] == 'score_accuracy')
    ]
    if not human_acc.empty:
        alpha = human_acc['krippendorff_alpha'].iloc[0]
        print(f"üìä Krippendorff's Œ± = {alpha:.3f}. Interpretation: {'POOR' if alpha < 0.4 else 'MODERATE'}.")
        if alpha < 0.2:
            print("   (CRITICAL FINDING: Value indicates agreement is close to or worse than chance, making the human 'gold standard' unreliable.)")
    else:
        print("   - Data not found.")

    # Q2a-Acc: Cross-Judge Agreement (Human vs. LLM)
    print("\nQ2a-Acc: Does the average human ranking of Accuracy match the average LLM-Judge ranking on RAG?")
    print("-" * 70)
    human_llm_acc = reliability_df[
        (reliability_df['comparison'] == 'Human_vs_LLM-Judge') &
        (reliability_df['metric'] == 'score_accuracy') & 
        (reliability_df['system'] == 'RAG')
    ]
    if not human_llm_acc.empty:
        tau = human_llm_acc['kendall_tau'].iloc[0]
        print(f"üìä Kendall's œÑ = {tau:.3f}. Interpretation: MODERATE agreement.")
    else:
        print("   - Data not found.")

    # Q3a-Acc: System Performance
    print("\nQ3a-Acc: Is RAG significantly more accurate than LLM-only when judged by 'gemini-1.5-flash' (LLM-Judge)?")
    print("-" * 70)
    sys_comp_acc = system_df[
        (system_df['judge_model'] == 'gemini-1.5-flash') &
        (system_df['judge_type'] == 'LLM-Judge') &
        (system_df['metric'] == 'score_accuracy')
    ]
    if not sys_comp_acc.empty:
        mean_diff = sys_comp_acc['mean_difference'].iloc[0]
        p_val = sys_comp_acc['wilcoxon_p_value'].iloc[0]
        significant = sys_comp_acc['wilcoxon_significant'].iloc[0]
        print(f"üìä Mean Difference = {mean_diff:.3f}. P-value = {p_val:.4f}. Significant = {significant}.")
        if significant and mean_diff > 0:
            print("   Interpretation: YES, RAG is significantly more accurate.")
        else:
            print("   Interpretation: NO, RAG is not significantly more accurate. In fact, it scored lower.")
    else:
        print("   - Data not found.")
        
    # Q4a-Acc: Judge Behavior
    print("\nQ4a-Acc: Are Agent-Judges stricter on Accuracy than LLM-Judges for the RAG system?")
    print("-" * 70)
    judge_acc_rag = judge_df[
        (judge_df['metric'] == 'score_accuracy') & 
        (judge_df['system'] == 'RAG')
    ]
    if not judge_acc_rag.empty:
        strict_judges = judge_acc_rag[judge_acc_rag['paired_mean_diff'] < 0]
        print(f"üìä Across {len(judge_acc_rag)} models, {len(strict_judges)} showed Agent-Judges as stricter.")
        # Check for any significant differences and report the most notable one
        significant_diffs = judge_acc_rag[judge_acc_rag['paired_t_sig'] == True]
        if not significant_diffs.empty:
            # Find the largest absolute difference
            largest_diff_idx = significant_diffs['paired_mean_diff'].abs().idxmax()
            largest_diff_row = significant_diffs.loc[largest_diff_idx]
            model = largest_diff_row['judge_model']
            diff = largest_diff_row['paired_mean_diff']
            direction = "LLM-Judge" if diff > 0 else "Agent-Judge"
            print(f"   (NOTE: A significant difference was found for '{model}', with {direction} scoring {abs(diff):.3f} points higher).")
        else:
            print("   (No significant differences found between judge types for accuracy on RAG system).")

def deep_dive_relevance(dfs):
    """Performs and prints the deep-dive analysis for the Relevance dimension."""
    print("\n\n" + "="*80)
    print("üéØ DEEP DIVE: RELEVANCE DIMENSION")
    print("="*80)
    
    reliability_df = dfs['reliability']
    system_df = dfs['system']
    judge_df = dfs['judge']

    # Q1a-Rel: LLM-Judge Consistency
    print("\nQ1a-Rel: How consistent is the LLM-Judge trio on Relevance for the LLM-only system?")
    print("-" * 70)
    llm_rel_llmonly = reliability_df[
        (reliability_df['analysis_type'] == 'llm_judge_reliability') &
        (reliability_df['metric'] == 'score_relevance') & 
        (reliability_df['system'] == 'LLM-only')
    ]
    if not llm_rel_llmonly.empty:
        alpha = llm_rel_llmonly['krippendorff_alpha'].iloc[0]
        print(f"üìä Krippendorff's Œ± = {alpha:.3f}. Interpretation: GOOD reliability.")
    else:
        print("   - Data not found.")

    # Q2a-Rel: Cross-Judge Agreement (LLM vs Agent)
    print("\nQ2a-Rel: Do LLM-Judge and Agent-Judge rankings match on Relevance?")
    print("-" * 70)
    cross_rel = reliability_df[
        (reliability_df['comparison'] == 'LLM-Judge_vs_Agent-Judge') &
        (reliability_df['metric'] == 'score_relevance')
    ]
    if not cross_rel.empty:
        for _, row in cross_rel.iterrows():
            tau = row['kendall_tau']
            system = row['system']
            print(f"  - {system} System: Kendall's œÑ = {tau:.3f} (MODERATE agreement).")
    else:
        print("   - Data not found.")

    # Q3a-Rel: System Performance (Robustness)
    print("\nQ3a-Rel: Does the Relevance gain from RAG hold across all Agent-Judges?")
    print("-" * 70)
    sys_rel_agent = system_df[
        (system_df['judge_type'] == 'Agent-Judge') &
        (system_df['metric'] == 'score_relevance')
    ]
    if not sys_rel_agent.empty:
        significant_gains = sys_rel_agent[sys_rel_agent['wilcoxon_significant'] & (sys_rel_agent['mean_difference'] > 0)]
        print(f"üìä Found {len(significant_gains)}/{len(sys_rel_agent)} Agent-Judges with a significant RAG advantage.")
        print("   Interpretation: The gain is NOT ROBUST, as it does not hold across the majority of judges.")
    else:
        print("   - Data not found.")
        
    # Q4a-Rel: Judge Behavior (Consistency)
    print("\nQ4a-Rel: Is the Relevance gap between judge types consistent across all models?")
    print("-" * 70)
    judge_rel = judge_df[judge_df['metric'] == 'score_relevance']
    if not judge_rel.empty:
        significant_gaps = judge_rel[judge_rel['paired_t_sig']]
        print(f"üìä Found significant gaps for {len(significant_gaps)} out of {len(judge_rel)} total comparisons.")
        print("   Interpretation: The gap is PARTIALLY SYSTEMATIC, appearing for some models but not others.")
    else:
        print("   - Data not found.")

def surface_analysis_completeness_overall(dfs):
    """Performs a surface-level analysis of Completeness and Overall scores."""
    print("\n\n" + "="*80)
    print("üéØ SURFACE ANALYSIS: COMPLETENESS & OVERALL DIMENSIONS")
    print("="*80)
    
    judge_df = dfs['judge']
    system_df = dfs['system']

    # Completeness Bias
    print("\nKey Finding: Completeness")
    print("-" * 70)
    comp_judge = judge_df[judge_df['metric'] == 'score_completeness']
    if not comp_judge.empty:
        strongest_bias = comp_judge.loc[comp_judge['paired_cohens_d'].abs().idxmax()]
        model = strongest_bias['judge_model']
        d_val = strongest_bias['paired_cohens_d']
        p_val = strongest_bias['paired_t_p']
        print(f"üìä LLM-Judges show systematic bias toward comprehensive responses, most strongly with '{model}' (d={d_val:.2f}, p<{p_val:.3f}).")
    else:
        print("   - Data not found.")

    # Overall System Performance
    print("\nKey Finding: Overall")
    print("-" * 70)
    overall_sys = system_df[system_df['metric'] == 'score_overall']
    if not overall_sys.empty:
        rag_wins = overall_sys[overall_sys['wilcoxon_significant'] & (overall_sys['mean_difference'] > 0)]
        print(f"üìä No significant RAG superiority on overall scores ({len(rag_wins)}/{len(overall_sys)} wins) confirms dimension-specific findings.")
    else:
        print("   - Data not found.")

def main():
    """Main function to run the focused analysis pipeline."""
    print("Running Focused Analysis Script...")
    dfs = load_data()
    if dfs:
        deep_dive_accuracy(dfs)
        deep_dive_relevance(dfs)
        surface_analysis_completeness_overall(dfs)
        print("\n\n‚úÖ Focused analysis complete.")

if __name__ == "__main__":
    main()